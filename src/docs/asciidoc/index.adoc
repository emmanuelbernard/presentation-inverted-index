= Inverted index
Emmanuel Bernard
2016-12-20
:hardbreaks:
:revnumber: {project-version}
:example-caption!:
ifndef::imagesdir[:imagesdir: images]
ifndef::sourcedir[:sourcedir: ../java]
:deckjs_transition: fade
:revealjs_slidenumber: false
:navigation:
:menu:
:status:
:stem:

== What are we here for

Discuss inverted index technology and fundamentals

=== Emmanuel Bernard

++++
<style>
.asciinema-terminal.font-medium {
  font-size: 16px;
}
</style>
++++

Consulting Software Engineer, Red Hat
Platform Architect
Open Source coder:

* Hibernate {ORM,OGM,Search,Validator}
* Debezium, Infinispan, Ceylon, ...

Podcasts: https://lescastcodeurs.com:[Les Cast Codeurs]

https://twitter.com/emmanuelbernard[@emmanuelbernard]
https://emmanuelbernard.com

=== Agenda

Why
B-tree vs inverted index
Building & using an inverted index
Indexing
Querying
Scoring
Physical representation


== Why do we need inverted index

=== Where is it used

[.left]
--
Classical:

* Google, DuckDuckGo
* Mobile phone search
* IDE  auto-completion
--

[.right]
--
Less classical

* geolocation
* suggestion
* faceting
* more like this / classification
* machine learning
--

=== Let's try on RDBMSes

[source,SQL]
----
SELECT * FROM Speakers s WHERE s.bio = 'Open Source'
----

How is the index built?

[NOTE.speaker]
--
Who knows SQL?

Discuss table full scan
How is the index built?
Discuss B-tree
--

[.fundamental]
== A zoom on B-Trees

Self-balancing tree data stucture
Search, insert and delete in `O(log n)`
Good when access time >> process time

Replace numbers in the example with letters for text.

image::b-tree/b-tree-base.png[]

[NOTE.speaker]
--
Complexity

Ask when is access time >> process time
hard drive (spinning and SSD)

Order 5 tree
Explain how to read from the tree
explain data is where the number is
--

[.fundamental]
=== Splitting leaf nodes

[.left]
--
image::b-tree/b-tree-base.png[]
--

[.right]
--
image::b-tree/b-tree-1.png[]
--

[NOTE.speaker]
--
Overflow
Split in two "in the middle"
move the middle up
--

[.fundamental]
=== Simple addition

[.left]
--
image::b-tree/b-tree-1.png[]
--

[.right]
--
image::b-tree/b-tree-2.png[]
--

[.fundamental]
=== Splitting nodes

[.left]
--
image::b-tree/b-tree-2.png[]
--

[.right]
--
image::b-tree/b-tree-3.png[]
--

[.fundamental]
=== Filling up a leaf node

[.left]
--
image::b-tree/b-tree-3.png[]
--

[.right]
--
image::b-tree/b-tree-4.png[]
--

[.fundamental]
=== Splitting nodes and filling up the root

[.left]
--
image::b-tree/b-tree-4.png[]
--

[.right]
--
image::b-tree/b-tree-5.png[]
--

[.fundamental]
=== Filling up leaves

[.left]
--
image::b-tree/b-tree-5.png[]
--

[.right]
--
image::b-tree/b-tree-6.png[]
--

[.fundamental]
=== Adding one level

[.left]
--
image::b-tree/b-tree-6.png[]
--

[.right]
--
image::b-tree/b-tree-7.png[]
--

[NOTE.speaker]
--
Show which node has been changed and which node has not changed

Removing: nodes cannot have 1 entry (except top)
--

[.fundamental]
=== B+-tree

Only keys in the non leaf nodes
Leaf nodes linked with one another for efficient ascending reading
Data can be just pointer to the real data

=== XKCD: Tree

image::b-tree/xkcd-tree.png[link:"https://www.xkcd.com/835/"]

Not only is that terrible in general, but you just KNOW Billy's going to open the root present first, and then everyone will have to wait while the heap is rebuilt.



== Back to our (B-Tree) RDBMS vs inverted indices

=== With like

[source,SQL]
----
SELECT * FROM Speakers s WHERE s.bio LIKE 'Open Source%'
----

With like we can have more text after
Still using indices

=== With like in the middle of the column

[source,SQL]
----
SELECT * FROM Speakers s WHERE s.bio LIKE '%Open Source%'
----

Find word anywhere in the text

Table or index scan :(

=== What about uppercase, typos etc

[source,SQL]
----
SELECT * FROM Speakers s WHERE
    s.bio LIKE '%open source%'
    OR s.bio LIKE '%Open Source%'
    OR s.bio LIKE '%opan surce%'
----

Can't anticipate the casing
Can't anticipate all typos

=== What about word ordering and priority

[source,SQL]
----
SELECT * FROM/Speakers s WHERE
    s.bio LIKE '%source open%'
    OR s.bio LIKE '%source%'
    OR s.bio LIKE '%open%'
    ORDER BY best??
----

Words could be in any order
I want the most interesting result first

=== Caveat on RDBMSes

Some have powerful indexing techniques
Some even full-text related

Tend to have less flexibility than dedicated inverted index tools

== Building & using an inverted index

=== Inverted index to the rescue

Let's not index column values but words
Let's not query values but words

[NOTE.speaker]
--
I will use word, token and term interchangeably.
Terms are the token + the field name in Lucene
--

=== At indexing time

[.left.small]
--
doc1: I am your father Luke
doc2: Yes he is your father
doc3: I am gonna make him an offer he can not refuse.
doc4: I love the smell of napalm in the morning.
doc5: One morning I shot an elephant in my pajamas. How he got in my pajamas, I do not know.
--

[.right.small]
--
|===
|word|documents

|am|1,3
|an|3,5
|can|3
|do|5
|elephant|5
|father|1,2
|gonna|3
|got|5
|he|2,3,5
|him|3
|how|5
|i|1,3,4,5
|in|4,5
|is|2
|know|5
|love|4
|luke|1
|make|3
|morning|4,5
|my|5
|not|3,5
|napalm|4
|of|4
|offer|3
|one|5
|pajamas|5
|refuse|3
|shot|5
|smell|4
|the|4
|yes|2
|your|1,2
|===
--

=== At query time

`query: father napalm`
Apply the same word splitting logic
Matching documents: 1, 2 and 4

|===
|word|documents

|father|1,2
|napalm|4
|===



== Indexing details

=== Transforming sentences into words

Analyzers

1. pre-tokenization
2. tokenization
3. filter

Apply the same logic to both document and query content
Each token is the entry in the inverted index pointing to documents

[NOTE.speaker]
--
Using the same analyzer stack for index and query is important
That's the key the index / the map is accessed by

If not using the same analyzer => won't find a match
--

=== Pre-tokenization

Remove unnecessary characters
e.g. remove HTML tags

[source]
----
<p>This is <strong>awesome</strong>.</p>
This is awesome.
----

=== Tokenization

Split sentence into words called _tokens_
Split at spaces, dots and other punctuations (with exceptions)

`aujourd'hui`, `A.B.C.`, and many other rules

One tokenizer per language, but many languages are similar

[.aside]
=== Continuous scripting

Didyouknowwritingtextsinwordsseparatedbyspaceisnotthatold
itstartedinthemiddleage
Itwasnotaproblemaspeoplewerereadingoutloudwrittentext
Infactsplittingwordswasaninventionnecessary
becausemonksshouldremainsilentandlatinwasnolongertheirnativetongue

[NOTE.speaker]
--
Mention that tokenizer does not necesseraly work with the notion of words (Chinese)
--

=== Filtering: where the magic happens

Operate on the stream of tokens
Change, remove or even add tokens

lowercase, stopwords

[source]
--
Sentence: This is AWESOME Peter!
Tokens: |This|is|AWESOME|Peter|
stopwords: |AWESOME|Peter|
lowercase: |awesome|peter|
--

=== Solving various problems with filters

=== Synonyms

When the text mentions a "car" but the research is about "automobile" or "vehicle"
We need a synonym dictionary

=== Synonym solution

1. Put all synonyms in the index for each word
2. Use a reference synonym ("automobile" for "car", "compact", "auto", "S.U.V."...)
3. Index normally, use synonyms when building the query

[NOTE.speaker]
--
Discuss the pros and cons.
Esp 3 is more agile (no need to reindex) but more work at query time.
--

=== Words from the same family

`education`, `educates`, `educated`, ...
That would make for lots of synonyms...
Let's use a stemming algorithm

=== An algorithm to copy language logic (and exceptions)

[.left]
--
Porter stemming algorithm
Snowball grammar
http://snowballstem.org/algorithms/french/stemmer.html[French algorithm explained]

Index/query the stem when the word is found
--

[.right]
--
[.french]
|===
|word|stem

|main|main
|mains|main
|maintenaient|mainten
|maintenait|mainten
|maintenant|mainten
|maintenir|mainten
|maintenue|mainten
|maintien|maintien
|===
--

[NOTE.speaker]
--
Porter stemming algorithm 1979, one of the oldest and widely used
Snowball is string processing programming language to build stemming algorithms
--

=== Finding words with typos

People make mistakes
In the text or in the query

They make _thaipo_ and other _mystakes_

[NOTE.speaker]
--
Ask them for possible approaches

* phonetic
* ngram
* fuzzy
--

=== Phonetic algorithm

Same logic as stemming, convert word into phonetic approximation
Soundex, RefinedSoundex, Metaphone, DoubleMetaphone

[NOTE.speaker]
--
* Soundex most well known and oldest
* RefinedSoundex more focused on spell checking
* Metaphone: variable length phonetic approximation
* Double Metaphone: handles more irregularities from English, German, Greek, French, Chinese

Phonetic algorithms relatively costly
--

=== n-gram

Split a word into a sliding window of n characters
Index each n-gram

[source]
--
// building a 3 gram
mystake: mys yst sta tak ake
mistake: mis ist sta tak ake
--

Low n means more false positives
High n means less forgiving

=== Fuzzy search

Based on Damerau-Levenshtein distance

* insert, update, delete and transposition

Pure query time operation

[NOTE.speaker]
--
Levenshtein: only insert, update, delete
Damerau: adds transposition of adjacent characters (i.e. swapping)
swapping: 80% of misspelling

Also used for protein sequence
--

=== Fuzzy search in practice

Compute distance between word and all words in index
or
Compute a distance state machine for word
Use it to check specific terms in the index

[.left.small]
--
n^e^: n consumed chars, e errors
horizontal: unmodified chars
* vertical: addition
* diagonal: substitution
Îµ diagonal: deletion
--

[.right]
--
image::fuzzy/levenstein-nfa-food.png[]
--

[NOTE.speaker]
--
Read https://julesjacobs.github.io/2015/06/17/disqus-levenshtein-simple-and-fast.html and http://blog.notdot.net/2010/07/Damn-Cool-Algorithms-Levenshtein-Automata
The image is a Non deterministic Finite Automaton
--

=== You can index the same data in different ways

Apply different indexing approach for same data

== Querying time

It's _term_ query all the way down!
All queries (synonyms, phonetic, n-gram, fuzzy) are a (set of) term queries

=== Possible queries

Term, wildcard, prefix, fuzzy, phrase, range, boolean, all, spatial, more like this, spell checking

=== PhraseQuery vs shingles

Find exact sentences
or find words near one another (sloppiness)

[source]
----
"Laurel and Hardy"
----

PhraseQuery uses positional information

Shingles uses n-grams but per tokens not per chars

[NOTE.speaker]
--
Phrase query will find all documents matching all terms,
Then request the positional information
then decides whether it matches or not.
Slop factor is the "edit distance" per word permutation

Shingles is simply the idea of indexing 2 or more words in one token

* faster
* more space
--

== Scoring

[.left]
--
image::scoring/xkcd-scoring.png[link="https://xkcd.com/1334/]
--

[.right]
--
Found results but in random order...

We want the most relevant results first
This is relative
Several approaches, none perfect
--

=== Main levers for a scoring formulae

Term frequency::
How often does the term appear in this document?
More is better

Inverse document frequency::
How often does the term appear in all documents in the collection?
Common words are less important

Field-length norm::
How long is the field?
Long documents would be favored otherwise

Coordination factor::
If document contains multiple terms, it's a better fit.

[%step]
=== TF/IDF Full formulae

[stem.small]
++++
"score"(q,d) =
    "queryNorm"(q)
    * "coord"(q,d)
    * sum_(t in q) (
        tf(t in d)
        * idf(t)^2
        * "t.boost"
        * "norm"(d)
    )
++++

[stem.small]
++++
"queryNorm"(q) = 1/sqrt(sum_(t in q) (idf(t)^2))
++++

[stem.small]
++++
"coord"(q,d) = ("nbrOfmatchingTerm"(q in d))/("nbrOfTerms"(q))
++++

[stem.small]
++++
tf(t in d) = sqrt(nbrOfTermAppearance(t in d))
++++

[stem.small]
++++
idf(t) = 1 + log ( "numDocs" / ("numDocs"(t in d) + 1))
++++

[stem.small]
++++
"norm"(d) = 1/sqrt( "nbrOfTerms"(d) )
++++

[NOTE.speaker]
--
norm:: is field normalization
t.boost:: is the term query boost
query normalization factor:: an attempt to normalize a query so that the results from one query may be compared with the results of another
coord:: nbr of matching terms in a query present in the document / number of terms
--

=== Other scoring

Boosting fields
Positional (phrase query) or similarity (fuzzy) information
Feedback function (external or internal)

Okapi BM25
Your custom scoring function (or a tweak of)


== Inverted index physical representation

A Lucene example

=== What is Lucene

Search engine library
Used by many, including

* Solr
* Elasticsearch
* Hibernate Search

[.fundamental]
=== B-tree's problems

When you need write throughput
B-tree requires lots of updates in place

Sequential reads are much faster than random reads

* on disk
* kinda also in memory (L1-3 caches)

[NOTE.speaker]
--
Updates in place means locking the structure while being updated
Not ideal for scalability
--

[.fundamental]
=== Append logs

Append operations in a file
Reading requires reading all the log

[.fundamental]
=== Log-Structured Merge

Per batch of writes, create a file storing the sorted key/value pairs
On read, check for the key on each file
Regularly merge files together (e.g. make bigger files)

image::lsm/lsm-base.png[Log-Structured Merge Tree]

[NOTE.speaker]
--
Explain how a reads proceeds from in memory to the most recent generation then going back in time
Mention tombstones for deletes
--

[.fundamental]
=== LSM characteristics

Immutable (lock-free) and file cache friendly
Fast on write, decent on read
Sequential read/write friendly
Read time decays with number of files => merge

[.fundamental]
=== Lots of ways to improve them

Page index in memory
Bloom filter
Level-based compaction

[NOTE.speaker]
--
Page index in memory:: put a memory efficient index for each entries in each LSM file
Bloom filter:: probabilistic data structure, false positive but no false negative https://en.wikipedia.org/wiki/Bloom_filter
--

[.fundamental]
=== level-based compaction for LSM tree

image::lsm/lsm-levelled-compaction.png[Log-Structured Merge Tree]

TODO: improve image

[NOTE.speaker]
--
keep in memory buffer
First level is like a LSM we discussed
Other levels are ranged (by key)
Compaction from one level to the higher will rebuilt
https://emmanuelbernard.com/blog/2017/01/10/lsm-tree-with-level-based-compaction/

Show how many files to read
--

[.fundamental]
=== level-based compaction characteristics

Limit the number of files to read
One file per level to be consulted
Compact to the higher levels
Each file per level has non overlapping key ranges

=== Lucene's case

LSM
Everything is computed upfront
Each _segment_ is a mini index
Denormalize differently depending on access pattern

=== A segment (simplified)

* term index (like a ToC for the dictionary)
* term dictionary (points to posting list offset)
* posting list (list of matching document id per term)
* stored field index (sparse doc id + offset)
* stored field data (list of field values per document id)
* deleted documents

=== Term index

Term index provides offset to the dictionary
Based on _finite state transducers_
Gives one ordinal per prefix

We know where to look in the term dictionary

[.left]
--
image::file-structure/FSTExample.png[]
--

[.right.small]
--
FST for mop, moth, pop, star, stop and top

[source]
----
mop=0
moth=1
pop=2
star=3
stop=4
top=5
----
--


[NOTE.speaker]
--
Thanks to immutable, can be built at merge time
Thanks to immutable, replace term with its ordinal value and index in a virtual array
Terms are ordered alphabetically and given an ordinal => alter comparison by ordinal comparison

FST: each arc has a letter and a weight (defaults 0)
Retrieve the offset in the term dictionary (sparse numbers)
--

=== Term dictionary

From a given offset (& prefix)
Sorted list of suffixes
For each, frequency and offset to posting list

[source]
----
[prefix=top]
_null_, freq=27, offset=234
ography, freq=1, offset=298
ology, freq=6, offset=306
onyms, freq=1, offset=323
----

=== Posting list

List of document ids
Encoded as delta of ids (good for variable int encoding)

[source]
----
4,6,9,30,33,39,45 => 4,2,3,23,3,6,6
----

http://www2008.org/papers/pdf/p387-zhangA.pdf[PForDelta] encoding
Bigger in size but less CPU branch miss prediction

[NOTE.speaker]
--
PForDelta
By batch of 128 integers, find the smallest number of bits for the biggest int
And use this as fixed encoding.
Note that it has a notion of exception for ints bigger than b bits to improve the logic

Also if you list is not a multiple of 128, they store the extra ones as variable ints (vint) in the end

Most important feature is not too much bigger
No branch misprediction inthe CPU
Better pipelining!

When practice make you look at other theories
Measure measure measure
--

=== Stored fields

Stored field index in memory doc id + offset for every 16k of data
Stored value stored as bocks of 16k and compressed

image::file-structure/stored-fields.png[]

[NOTE.speaker]
--
Index is the upper part, all in memory.
Binary search

Each block of 16k has a mini index at the beginning to go to the right doc
and each doc is concatenated key/value pairs

Each block is compressed with LZ4
--

=== Deleted documents

You said segments are immutable
What about deleted documents?

Deleted document file

1. 1 bit per doc
2. sparse list of cleared docs

[NOTE.speaker]
--
Only mutable part of the segment
And only one way (from present to not present)
--

=== Why oh why such a mess?

image::file-structure/xkcd-lisp.jpg[link="https://xkcd.com/224/"]

2 disk seeks per field search (binary search)
1 disk seek per doc for stored fields

But things likely fit in file system cache

Warning: this is a simplified view :)

[NOTE.speaker]
--
Field search:

* term dict index is in memory
* 1 disk seek to reach the term dict
* 1 disk seek for the posting list

But in practice, term dict might be in file system cache and pulse optim
Pulse option: a term with 1 document, we don't store the id in posting list but in the term dict inlined

Stored field:

* index in memory
* one seek to the right block, read 16k
--


== Subjects not covered

=== Uninverted index

Columnar storage
Called doc values
Used for aggregation or sorting or faceting

=== Faceting

[NOTE.speaker]
--
Offer navigation within search results
Use doc values to efficiently implement it
--

=== Geospatial queries

[NOTE.speaker]
--
Several indexing techniques
hash indexing: make the world into increasingly smaller boxes
prefix query or not
--

=== Term vector

[NOTE.speaker]
--
More like this query

Store an inverted index per document id (freq, position)
--

=== And many more things


== Thank you!

* Slides https://emmanuelbernard.com/presentations/inverted-index/
* Code https://github.com/emmanuelbernard/presentation-inverted-index/
* Blog https://emmanuelbernard.com[emmanuelbernard.com]
* Follow me: http://twitter.com/emmanuelbernard[@emmanuelbernard]

=== License

image::intro/by-sa.png[link="http://creativecommons.org/licenses/by-sa/4.0/"]
This work is licensed under a http://creativecommons.org/licenses/by-sa/4.0/[Creative Commons Attribution-ShareAlike 4.0 International License].

https://xkcd.com[XKCD] images are licensed under http://creativecommons.org/licenses/by-nc/2.5/[Creative Commons Attribution-NonCommercial 2.5 License].

A couple of drawings are copyright of their respective author (linked in the references).

=== References

[.small]
--
B-tree and LSM

* http://cis.stvincent.edu/html/tutorials/swd/btree/btree.html
* https://raw.githubusercontent.com/google/leveldb/master/doc/impl.html
* https://emmanuelbernard.com/blog/2017/01/10/lsm-tree-with-level-based-compaction/

Analyzers

* http://tartarus.org/~martin/PorterStemmer/
* snowballstem.org

Scoring

* https://modye.github.io/es-in-depth/#/4/19
* https://www.elastic.co/guide/en/elasticsearch/guide/current/practical-scoring-function.html
* https://en.wikipedia.org/wiki/Okapi_BM25
* https://speakerdeck.com/elastic/improved-text-scoring-with-bm25

Query

* https://julesjacobs.github.io/2015/06/17/disqus-levenshtein-simple-and-fast.html
* http://blog.mikemccandless.com/2011/03/lucenes-fuzzyquery-is-100-times-faster.html
* http://blog.notdot.net/2010/07/Damn-Cool-Algorithms-Levenshtein-Automata
--

[%notitle]
=== References 2

[.small]
--
Lucene file and memory structure

* http://stackoverflow.com/questions/2602253/how-does-lucene-index-documents
* https://web.archive.org/web/20130904073403/http://www.ibm.com/developerworks/library/wa-lucene/
* http://lucene.apache.org/core/4_10_2/core/org/apache/lucene/codecs/lucene410/package-summary.html#package_description
* http://lucene.apache.org/core/3_6_2/fileformats.html
* https://youtu.be/T5RmMNDR5XI
* http://www.research.ibm.com/haifa/Workshops/ir2005/papers/DougCutting-Haifa05.pdf
* http://blog.mikemccandless.com/2010/12/using-finite-state-transducers-in.html
* http://blog.parsely.com/post/1691/lucene/
--
